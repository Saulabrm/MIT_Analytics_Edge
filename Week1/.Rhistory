pt(.99,5)
pt(.01,5)
pt(.01,5)/2
pnorm(2.575,lower.tail=FALSE)
pnorm(2.575,lower.tail=TRUE)
?pnorm
qnorm(.99,lower.tail=FALSE)
qnorm(.99,lower.tail=TRUE)
qnorm(.05,lower.tail=TRUE)
qnorm(.05,lower.tail=FALSE)
qnorm(.999,lower.tail=FALSE)
qnorm(.99,lower.tail=TRUE)
qnorm(.99,lower.tail=TRUE)
15.4+2.365*.5
15.4-2.365*.5
1.645*.5/.05
16.45^2
1 - pchisq(12.6, 5)
qchisq(.95, df=5)
choose(40,3)
1-choose(40,3)/choose(52,3)
qnorm(.9,lower.tail=TRUE)
qnorm(.95,lower.tail=TRUE)
qnorm(.9,lower.tail=FALSE)
qnorm(.95,lower.tail=FALSE)
qnorm(.95,lower.tail=TRUE)
qnorm(.9,lower.tail=TRUE)
qnorm(.9,lower.tail=TRUE)
qnorm(.95,lower.tail=TRUE)
dbinom(c(9:17),size=17,prob=.552)
sum(dbinom(c(9:17),size=17,prob=.552))
qnorm(.975,lower.tail=TRUE)
z_normal <- function(x){
y = 1 - pnorm(x)
return(y)
}
z_normal(1.96)
z_normal(.05)
z_normal(.95)
z_normal(.975)
z_normal(.05)
z_normal(.5)
z_normal(5)
z_normal(.63)
z_normal(.353)
z_normal(2.08)
pt(.95,8)
?pt
homes1<-c(340,344,362,375)
homes2<-c(356,386,354,364)
homes3<-c(332,402,340,355)
homes4<-c(318,360,338,370)
homes<-rbind(homes1,homes2,homes3,homes4)
homes
homes5<-c(318,360,338,370)
homes4<-c(362,322,372,324)
homes<-rbind(homes1,homes2,homes3,homes4,homes5)
homes
mean(homes)
?var
var(homes)
sd(homes)
?apply
(homes-350)^2
sum((homes-350)^2)
sum((homes-350)^2)/8
sqrt(sum((homes-350)^2)/8)
sqrt(sum((homes-350)^2)/9)
sqrt(sum((homes-350)^2)/8)
z_normal(.333)
sd(homes)
Z= (mean(homes)-350)/(sd(homes)/sqrt(9))
Z
mean(homes)
sd(homes)/3
3.8/sd(homes/3)
z_normal(.5217917)
library(ISLR)
summary(Hitters)
Hitters=na.omit(Hitters)
with(Hitters, sum(is.na(Salary)))
summary(Salary)
library(leaps)
install.packages("leaps")
regfit.full= regsubsets(Salary~., data= Hitters)
summary(regfit.full)
library(leaps)
regfit.full= regsubsets(Salary~., data= Hitters)
summary(regfit.full)
regfut.full= regsubsets(Salary~.,data=Hitters, nvmax= 19)
reg.summary= summary(regfit.full)
reg.summary
names(reg.summary)
plot(reg.summary$cp, xlab="Number of variables", ylab="Cp")
which.min(reg.summary$cp)
points(10,reg.summary$cp[10,pch=20,col="red"])
points(10,reg.summary$cp[10],pch=20,col="red")
regfut.full= regsubsets(Salary~.,data=Hitters, nvmax= 19)
regfit.full= regsubsets(Salary~.,data=Hitters, nvmax= 19)
reg.summary= summary(regfit.full)
reg.summary
names(reg.summary)
plot(reg.summary$cp, xlab="Number of variables", ylab="Cp")
which.min(reg.summary$cp)
points(10,reg.summary$cp[10],pch=20,col="red")
plot(regfit.full, scale="Cp")
coef(regfit.full, 10)
dim(Hitters)
set.seed(1)
train=sample(seq(263), 180, replace=FALSE)
train
regfit.fwd= regsubsets(Salary~., data=Hitters[trains,], nvmax=19, method="forward")
regfit.fwd = regsubsets(Salary~., data=Hitters[-trains,], nvmax=19, method="forward")
val.errors=rep(NA,19)
x.text=model.matrix(Salary~.,data=Hitters[-train,]) # notice the -index
for(i in 1:19){
coefi=coef(regfit.fwd, id=i)
pred= x.test[,names(coefi)]%*%coefi
val.errors[i]= mean((Hitters$Salary[-train]-pred)^2)
}
dim(Hitters)
dim(Hitters)
set.seed(1)
train=sample(seq(263), 180, replace=FALSE)
train
regfit.fwd = regsubsets(Salary~., data=Hitters[-trains,], nvmax=19, method="forward")
regfit.fwd = regsubsets(Salary~., data=Hitters[train,], nvmax=19, method="forward")
val.errors=rep(NA,19)
x.text=model.matrix(Salary~.,data=Hitters[-train,]) # notice the -index
for(i in 1:19){
coefi=coef(regfit.fwd, id=i)
pred= x.test[,names(coefi)]%*%coefi
val.errors[i]= mean((Hitters$Salary[-train]-pred)^2)
}
plot(sqrt(val.errors), ylab="Root MSE", ylim = c(300,400), pch=19, type= "b")
points(sqrt(regfit.fwd$rss[-1]/180), col="blue", pch=19, type= "b")
legend("topright", legend=c("Training","Validation"), col=c("blue","black"), pch=19)
plot(sqrt(val.errors), ylab="Root MSE", ylim = c(300,400), pch=19, type= "b")
points(sqrt(regfit.fwd$rss[-1]/180), col="blue", pch=19, type= "b")
legend("topright", legend=c("Training","Validation"), col=c("blue","black"), pch=19)
val.errors=rep(NA,19)
x.test =model.matrix(Salary~.,data=Hitters[-train,]) # notice the -index
for(i in 1:19){
coefi=coef(regfit.fwd, id=i)
pred= x.test[,names(coefi)]%*%coefi
val.errors[i]= mean((Hitters$Salary[-train]-pred)^2)
}
plot(sqrt(val.errors), ylab="Root MSE", ylim = c(300,400), pch=19, type= "b")
points(sqrt(regfit.fwd$rss[-1]/180), col="blue", pch=19, type= "b")
legend("topright", legend=c("Training","Validation"), col=c("blue","black"), pch=19)
seq(263)
val.errors
x.test
val.errors=(NA,2)
val.errors=(NA,3)
val.errors=rep(NA,3)
val.errors
val.errors=rep(NA,5)
val.errors
val.errors=rep(NA,19)
x.test = model.matrix(Salary~.,data=Hitters[-train,]) # notice the -index
coef(regfit.fwd, id=i)
x.test[,names(coefi)]%*%coefi
set.seed(11)
folds<-sample(rep(1:10, length=nrow(Hitters)))
folds
table(folds)
cv.error=matrix(NA,10,19)
for(k in 1:10){
best.fit= regsubsets(Salary~., data=Hitters[folds!=k,], nvmax=19, method="forward")
for(i in 1:19){
pred=predict(best.fit, Hitters[folds==k,], id=i)
cv.errors[k,i]=mean((Hitters$Salary[folds==k]-pred)^2)
}
}
library(glmnet)
install.packages("glmnet")
library(glmnet)
x=model.matrix(Salary~.-1,data=Hitters)
y=Hitter$Salary
y=Hitters$Salary
fit.ridge= glmnet(x,y,alpha=0)
plot(fit.ridge, xvar="lambda", label=TRUE)
cv.ridge=cv.glmnet(x,y,alpha=0)
plot(cv.ridge)
fit.lasso=glmnet(x,y)
plot(fit.lasso, var="lambda", label=TRUE)
cv.lasso= cv.glmnet(x,y)
fit.lasso=glmnet(x,y)
lasso.tr= glmnet(x[train,], y[train])
lasso.tr
pred = predict(lasso.tr, x[-train,])
dim(pred)
lam.best=lasso.tr$lambda[order(rmse)[1]]
rmse= sqrt(apply((y[-train]-pred)^2,2,mean))
plot(log(lasso.tr$lambda), rmse, type="b", xlab="Log(lambda)")
lam.best=lasso.tr$lambda[order(rmse)[1]]
lam.best
coef(lasso.tr, s=lam.best)
head(Hitters)
head(Hitters[-1])
head(Hitters[,-1])
?sample
dim(Hitters)
?apply
```{r}
library(glmnet)
x=model.matrix(Salary~.-1,data=Hitters)
y=Hitters$Salary
```
First we will fit a ridge-regression model. This is achieved by calling `glmnet` with `alpha=0` (see the helpfile). There is also a `cv.glmnet` function which will do the cross-validation for us.
```{r}
fit.ridge= glmnet(x,y,alpha=0)
plot(fit.ridge, xvar="lambda", label=TRUE)
cv.ridge=cv.glmnet(x,y,alpha=0)
plot(cv.ridge)
```
Now we fit a lasso model; for this we use the default `alpha=1`
```{r}
fit.lasso=glmnet(x,y)
plot(fit.lasso, var="lambda", label=TRUE)
#deviance is the r squared
plot(fit.lasso, var="dev", label=TRUE)
cv.lasso= cv.glmnet(x,y)
plot(cv.lasso)
coef(cv.lasso)
```
Suppose we want to use our earlier train/validation division to select the `lambda` for the lasso.
This is easy to do.
```{r}
lasso.tr= glmnet(x[train,], y[train])
lasso.tr
pred = predict(lasso.tr, x[-train,])
dim(pred)
rmse= sqrt(apply((y[-train]-pred)^2,2,mean))
plot(log(lasso.tr$lambda), rmse, type="b", xlab="Log(lambda)")
#Gets the index of the least lambda
lam.best=lasso.tr$lambda[order(rmse)[1]]
lam.best
coef(lasso.tr, s=lam.best)
names(cv.lasso)
coef(cv.lasso)
cv.lasso
require(ISLR)
require(tree)
attach(Carseats)
hist(Sales)
High= ifelse(Sales<=8, "No", "Yes")
Carseats=data.frame(Carseats, High)
Carseats
tree.carseats= tree(High~.-Sales, data=Carseats)
summary(tree.carseats)
plot(tree.carseats)
require(tree)
install.packages("tree")
require(tree)
tree.carseats<- tree(High~.-Sales, data=Carseats)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats,pretty=0)
set.seed(1011)
train=sample(1:nrow(Carseats),250)
tree.carseats=tree(High~.-Sales, Carseats, subset=train)
plot(tree.carseats); text(tree.carseats,pretty=0)
tree.pred=predict(tree.carseats, Carseats[-train,], type="class")
with(Carseats[-train,] , table(tree.pred,High))
(72+33)/150
dim(Carseats)
dim(train)
sample(1:nrow(Carseats),250)
cv.carseats=cv.tree(tree.carseats, FUN= prune.misclass)
cv.carseats
plot(cv.carseats)
prune.carseats= prune.misclass(tree.carseats, best = 13)
plot(prune.carseats); text(prune.carseats,pretty=0)
tree.pred= predict(prune.carseats, Carseats[-train,], type="class")
cv.carseats=cv.tree(tree.carseats, FUN= prune.misclass)    # this does cross validation
cv.carseats
plot(tree.carseats); text(tree.carseats,pretty=0)
tree.pred=predict(tree.carseats, Carseats[-train,], type="class") #-train will leave our test observations, type="class" for labels
with(Carseats[-train,] , table(tree.pred,High))   #High is a column of Carseats, and we want the testing values
(72+33)/150  #Correct divided by all.
(72+32)/150
require(randomForest)
require(MASS)
set.seed(101)
dim(Boston)
train=sample(1:nrow(Boston),300)
?Boston
?detach(Carseats)
rf.boston=randomForest(medv~.,data=Boston, subset=train)
rf.boston
fit$mse
oob.err=double(13)
test.err=double(13)
for(mtry in 1:13){
fit= randomForest(medv~., data=Boston, subset=train, mtry=mtry, ntree=400)
oob.err[mtry]=fit$mse[400]
pred=predict(fit,Boston[-train,])
test.err[mtry]= with(Boston[-train,], mean((medv~pred)^2))
cat(mtry," ")
}
oob.err=double(13)
test.err=double(13)
for(mtry in 1:13){
fit= randomForest(medv~., data=Boston, subset=train, mtry=mtry, ntree=400)
oob.err[mtry]=fit$mse[400]
pred=predict(fit,Boston[-train,])
test.err[mtry]= with(Boston[-train,], mean((medv~pred)))
cat(mtry," ")
}
oob.err=double(13)
test.err=double(13)
for(mtry in 1:13){
fit= randomForest(medv~., data=Boston, subset=train, mtry=mtry, ntree=400)
oob.err[mtry]=fit$mse[400]
pred=predict(fit,Boston[-train,])
test.err[mtry]= with(Boston[-train,], mean((medv~pred)*(medv~pred)))
cat(mtry," ")
}
oob.err=double(13)
test.err=double(13)
for(mtry in 1:13){
fit= randomForest(medv~., data=Boston, subset=train, mtry=mtry, ntree=400)
oob.err[mtry]=fit$mse[400]
pred=predict(fit,Boston[-train,])
test.err[mtry]= with(Boston[-train,], mean((medv~pred)))
cat(mtry," ")
}
matplot(1:mtry,cbind(test.err, oob.err), pch=19, col=c("red","blue"), type="b",
ylab= "Mean Squared Error")
legend("topright",legend=c("OOB","Test"), pch=19, col=c("red","blue"))
matplot(1:mtry, cbind(test.err, oob.err), pch=19, col=c("red","blue"), type="b",
ylab= "Mean Squared Error")
legend("topright",legend=c("OOB","Test"), pch=19, col=c("red","blue"))
obb.err
oob.err
oob.err =double(13)
test.err =double(13)
for(mtry in 1 : 13){
fit=randomForest(medv~.,data=Boston,subset=train,mtry=mtry,ntree=400)
oob.err[mtry]=fit$mse[400]
pred=predict(fit,Boston[-train,])
test.err[mtry] = with(Boston[-train,],mean((medv~pred)*(medv~pred)))
cat(mtry," ")
}
matplot(1:mtry, cbind(test.err, oob.err), pch=19, col=c("red","blue"), type="b",
ylab= "Mean Squared Error")
legend("topright",legend=c("OOB","Test"), pch=19, col=c("red","blue"))
oob.err=double(13)
test.err=double(13)
for(mtry in 1:13){
fit= randomForest(medv~., data=Boston, subset=train, mtry=mtry, ntree=400)
oob.err[mtry]=fit$mse[400]
pred=predict(fit,Boston[-train,])
test.err[mtry]= with(Boston[-train,], mean((medv-pred)^2))
cat(mtry," ")
}
matplot(1:mtry, cbind(test.err, oob.err), pch=19, col=c("red","blue"), type="b",
ylab= "Mean Squared Error")
legend("topright",legend=c("OOB","Test"), pch=19, col=c("red","blue"))
require(gbm)
boost.boston=gbm(medv~., data=Boston[train,], distribution = "gaussian", n.trees=10000
, shrinkage=0.01, interaction.depth = 4)
summary(boost.boston)
plot(boost.boston, i="lstat")
plot(boost.boston, i="rm")
n.trees=seq(from=100, to=10000, by=100)
predmat= predict(boost.boston, newdata = Boston[-train], n.trees = n.trees)
dim(predmat)
predmat= predict(boost.boston, newdata = Boston[-train], n.trees = n.trees)
predmat = predict(boost.boston, newdata = Boston[-train], n.trees = n.trees)
predmat = predict(boost.boston, newdata = Boston[-train,], n.trees = n.trees)
dim(predmat)
berr=with(Boston[-train,],apply( (predmat-medv)^2,2,mean))
head(Boston)
n.trees=seq(from=100, to=10000, by=100)
predmat = predict(boost.boston, newdata = Boston[-train,], n.trees = n.trees)
dim(predmat)
berr=with(Boston[-train,],apply( (predmat-medv)^2,2,mean))
plot(n.trees, berr, pch=19, ylab="Mean Squared Error", xlab="# Trees", main= "Boosting Test Error")
abline(h=min(test.err),col="red")
matrix(nrow= 4, ncol= 8)
?apply
t?
?t
require(devtools )
install_github(&quot;andreacirilloac/updateR&quot;)
install_github(&quot;andreacirilloac/updateR&quot;)
version
setwd("/Users/saulgarcia/Desktop/Github/MOOCs/MIT Analyticals Edge/Week1")
#Problem 1.1 - Loading and Summarizing the Dataset
CPS = read.csv("CPSData.csv")
summary(CPS)
str(CPS)
#131302
#Problem 1.2 - Loading and Summarizing the Dataset
table(CPS$Industry)
table(CPS$Industry)[which.max(table(CPS$Industry))]
#Educational and health services
#Problem 1.3 - Loading and Summarizing the Dataset
sort(table(CPS$State))
#New Mexico and Carolina
#Problem 1.4 - Loading and Summarizing the Dataset
1-(table(CPS$Citizenship)[3]/nrow(CPS))
#Problem 1.5 - Loading and Summarizing the Dataset
table(CPS$Race,CPS$Hispanic)
#All but asian and pacific islander
#Problem 2.1 - Evaluating Missing Values
summary(CPS)
#Problem 2.2 - Evaluating Missing Values
table(CPS$Age, is.na(CPS$Married)) #Age
#Problem 2.3 - Evaluating Missing Values
table(CPS$State, is.na(CPS$MetroAreaCode)) # 2 and 3
#Problem 2.4 - Evaluating Missing Values
table(CPS$Region, is.na(CPS$MetroAreaCode))[,2]/(table(CPS$Region, is.na(CPS$MetroAreaCode))[,1]+table(CPS$Region, is.na(CPS$MetroAreaCode))[,2])
#Midwest
#Problem 2.5 - Evaluating Missing Values
tapply(is.na(CPS$MetroAreaCode),CPS$State, mean) #Wisconsin and Montana
#Problem 3.1 - Integrating Metropolitan Area Data
MetroAreaMap = read.csv("MetroAreaCodes.csv")
CountryMap = read.csv("CountryCodes.csv")
nrow(MetroAreaCode)
nrow(CountryMap)
#Problem 3.2 - Integrating Metropolitan Area Data
CPS = merge(CPS, MetroAreaMap, by.x="MetroAreaCode", by.y="Code", all.x=TRUE)
summary(CPS)
#MetroArea and 34238
#Problem 3.3 - Integrating Metropolitan Area Data
sort(table(CPS$MetroArea)) #Baltimore
#Problem 3.4 - Integrating Metropolitan Area Data
tail(sort(tapply(CPS$Hispanic,CPS$MetroArea, mean)),1)
CPS$Race == "Asian"
CPS[CPS$Race == "Asian",]
tail(sort(tapply(CPS$Hispanic,CPS$MetroArea, mean)),1)
table(CPS$Race,CPS$MetroArea)
table(CPS$MetroArea)
table(CPS$Race,CPS$MetroArea)
table(CPS$Race,CPS$MetroArea)[2,]
table(CPS$Race,CPS$MetroArea)
2,
table(CPS$Race,CPS$MetroArea)[2,]
table(CPS$Race,CPS$MetroArea)[2,]/table(CPS$MetroArea)
table(CPS$Race,CPS$MetroArea)[2,]/table(CPS$MetroArea) > .2
table(CPS$Race,CPS$MetroArea)[2,]/table(CPS$MetroArea) >= .2
df = table(CPS$Race,CPS$MetroArea)[2,]/table(CPS$MetroArea) >= .2
str(df)
as.data.frame(table(CPS$Race,CPS$MetroArea)[2,]/table(CPS$MetroArea) >= .2)
df=as.data.frame(table(CPS$Race,CPS$MetroArea)[2,]/table(CPS$MetroArea) >= .2)
head(df)
df[df$`table(CPS$Race, CPS$MetroArea)[2, ]/table(CPS$MetroArea) >= 0.2`==TRHE),]
df[df$`table(CPS$Race, CPS$MetroArea)[2, ]/table(CPS$MetroArea) >= 0.2`==TRUE),]
df[df$`table(CPS$Race, CPS$MetroArea)[2, ]/table(CPS$MetroArea) >= 0.2`==TRUE,]
sort(tapply(CPS$Race == "Asian", CPS$MetroArea, mean))
sort(tapply(CPS$Education == "No high school diploma", CPS$MetroArea, mean))
sort(tapply(CPS$Education == "No high school diploma", CPS$MetroArea, mean, na.rm=TRUE))
head(sort(tapply(CPS$Education == "No high school diploma", CPS$MetroArea, mean, na.rm=TRUE)))
names(CPS)
CPS = merge(CPS, CountryMap, by.x="CountryOfBirthCode", by.y="Code", all.x=TRUE)
names(CPS)
str(CPS)
summary(CPS)
table(CPS$Citizenship)
table(CPS$Country)
sort(table(CPS$Country))
sort(tapply(CPS$MetroArea == "New York-Northern New Jersey-Long Island, NY-NJ-PA", CPS$Country, mean, na.rm=TRUE))
CPS$MetroArea == "New York-Northern New Jersey-Long Island, NY-NJ-PA"
table(CPS$MetroArea == "New York-Northern New Jersey-Long Island, NY-NJ-PA",CPS$Country)
table(CPS$MetroArea == "New York-Northern New Jersey-Long Island, NY-NJ-PA",CPS$Country, na.rm=TRUE)
CPS[CPS$MetroArea == "New York-Northern New Jersey-Long Island,]
)
ç
]
CPS[CPS$MetroArea == "New York-Northern New Jersey-Long Island",]
head(CPS)
CPS[CPS$MetroArea == "New York-Northern New Jersey-Long Island" ,]
head(CPS)
table(CPS$MetroArea)
CPS[CPS$MetroArea == "New York-Northern New Jersey-Long Island, NY-NJ-PA" ,]
NY = CPS[CPS$MetroArea == "New York-Northern New Jersey-Long Island, NY-NJ-PA" ,]
summary(NY)
sort(tapply(NY$MetroArea, NY$Country, mean, na.rm=TRUE))
table(NY$MetroArea, NY$Country)
tail(table(CPS$Citizenship),3)  #Philippines
tail(table(CPS$Citizenship),3)  #Philippines
sort(table(CPS$Country))
tail(sort(table(CPS$Country)),3)  #Philippines
tail(sort(table(CPS$Country)),3)  #Philippines
table(NY$MetroArea, NY$Country == "United States")
1668 +  3736
table(NY$MetroArea, NY$Country == "United States")
table(CPS$MetroArea"New York-Northern New Jersey-Long Island, NY-NJ-PA", CPS$Country == "United States")
table(CPS$MetroArea=="New York-Northern New Jersey-Long Island, NY-NJ-PA", CPS$Country == "United States")
1668/(1668+3736)
sort(table(CPS$MetroArea, CPS$Country == "India"))
table(CPS$MetroArea, CPS$Country == "India")
max(table(CPS$MetroArea, CPS$Country == "India"))
max(table(CPS$MetroArea, CPS$Country == "India")[2])
max(table(CPS$MetroArea, CPS$Country == "India")[,2])
table(CPS$MetroArea, CPS$Country == "Brazil")
table(CPS$MetroArea, CPS$Country == "Brazil")[,2]
max(table(CPS$MetroArea, CPS$Country == "Brazil")[,2])
table(CPS$MetroArea, CPS$Country == "Somalia")[,2]
max(table(CPS$MetroArea, CPS$Country == "Somalia")[,2])
which.max(table(CPS$MetroArea, CPS$Country == "Somalia")[,2])
which.max(table(CPS$MetroArea, CPS$Country == "India")[,2])
which.max(table(CPS$MetroArea, CPS$Country == "Brazil")[,2])
tapply(c(CPS$MetroAreaCode),CPS$State,mean)
tapply(CPS$MetroAreaCode,CPS$State,mean)
tapply(CPS$MetroArea,CPS$State,mean)
tapply(CPS$MetroArea,CPS$State,mean, na.rm=TRUE)
tapply(c(CPS$MetroArea),CPS$State, mean, na.rm=TRUE)
