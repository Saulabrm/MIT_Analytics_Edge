ylab= "Mean Squared Error")
legend("topright",legend=c("OOB","Test"), pch=19, col=c("red","blue"))
matplot(1:mtry, cbind(test.err, oob.err), pch=19, col=c("red","blue"), type="b",
ylab= "Mean Squared Error")
legend("topright",legend=c("OOB","Test"), pch=19, col=c("red","blue"))
obb.err
oob.err
oob.err =double(13)
test.err =double(13)
for(mtry in 1 : 13){
fit=randomForest(medv~.,data=Boston,subset=train,mtry=mtry,ntree=400)
oob.err[mtry]=fit$mse[400]
pred=predict(fit,Boston[-train,])
test.err[mtry] = with(Boston[-train,],mean((medv~pred)*(medv~pred)))
cat(mtry," ")
}
matplot(1:mtry, cbind(test.err, oob.err), pch=19, col=c("red","blue"), type="b",
ylab= "Mean Squared Error")
legend("topright",legend=c("OOB","Test"), pch=19, col=c("red","blue"))
oob.err=double(13)
test.err=double(13)
for(mtry in 1:13){
fit= randomForest(medv~., data=Boston, subset=train, mtry=mtry, ntree=400)
oob.err[mtry]=fit$mse[400]
pred=predict(fit,Boston[-train,])
test.err[mtry]= with(Boston[-train,], mean((medv-pred)^2))
cat(mtry," ")
}
matplot(1:mtry, cbind(test.err, oob.err), pch=19, col=c("red","blue"), type="b",
ylab= "Mean Squared Error")
legend("topright",legend=c("OOB","Test"), pch=19, col=c("red","blue"))
require(gbm)
boost.boston=gbm(medv~., data=Boston[train,], distribution = "gaussian", n.trees=10000
, shrinkage=0.01, interaction.depth = 4)
summary(boost.boston)
plot(boost.boston, i="lstat")
plot(boost.boston, i="rm")
n.trees=seq(from=100, to=10000, by=100)
predmat= predict(boost.boston, newdata = Boston[-train], n.trees = n.trees)
dim(predmat)
predmat= predict(boost.boston, newdata = Boston[-train], n.trees = n.trees)
predmat = predict(boost.boston, newdata = Boston[-train], n.trees = n.trees)
predmat = predict(boost.boston, newdata = Boston[-train,], n.trees = n.trees)
dim(predmat)
berr=with(Boston[-train,],apply( (predmat-medv)^2,2,mean))
head(Boston)
n.trees=seq(from=100, to=10000, by=100)
predmat = predict(boost.boston, newdata = Boston[-train,], n.trees = n.trees)
dim(predmat)
berr=with(Boston[-train,],apply( (predmat-medv)^2,2,mean))
plot(n.trees, berr, pch=19, ylab="Mean Squared Error", xlab="# Trees", main= "Boosting Test Error")
abline(h=min(test.err),col="red")
matrix(nrow= 4, ncol= 8)
?apply
t?
?t
require(devtools )
install_github(&quot;andreacirilloac/updateR&quot;)
install_github(&quot;andreacirilloac/updateR&quot;)
version
version
setwd("/Users/saulgarcia/Desktop/Github/MIT_Analytics_Edge/Week5")
wiki <- read.csv("wiki.csv", stringsAsFactors = FALSE)
wiki$Vandal <- as.factor(wiki$Vandal)
table(wiki$Vandal)
table(wiki$Vandal)[2]
library(tm)
str(wiki)
?strwrap
corpusAdded = Corpus(VectorSource(wiki$Added))
corpusAdded = tm_map(corpus, removeWords, stopwords("english"))
corpusAdded = tm_map(corpusAdded, removeWords, stopwords("english"))
corpusAdded = tm_map(corpusAdded, stemDocument)
dtmAdded = DocumentTermMatrix(corpusAdded)
length(stopwords("english"))
dtmAdded
sparseAdded = removeSparseTerms(dtmAdded, 0.997) #Remove sparse terms that dont appear in at least 3% of the document
sparseAdded
wordsAdded = as.data.frame(as.matrix(sparseAdded))
str(wordsAdded)
colnames(wordsAdded) = paste("A", colnames(wordAdded))
colnames(wordsAdded) = paste("A", colnames(wordsAdded))
str(wordsAdded)
corpusRemoved = tm_map(corpusRemoved, removeWords, stopwords("english"))
corpusRemoved = Corpus(VectorSource(wiki$Removed))
corpusRemoved = tm_map(corpusRemoved, removeWords, stopwords("english"))
corpusRemoved = tm_map(corpusRemoved, stemDocument)
dtmRemoved = DocumentTermMatrix(corpusRemoved)
sparseRemoved = removeSparseTerms(dtmRemoved)
dtmRemoved = DocumentTermMatrix(corpusRemoved)
sparseRemoved = removeSparseTerms(dtmRemoved)
sparseRemoved = removeSparseTerms(dtmRemoved, 0.997)
wordsRemoved = as.data.frame(as.matrix(sparseRemoved))
head(wordsRemoved)
str(wordsRemoved)
colnames(wordsRemoved) = paste("R", colnames(wordsRemoved))
dim(wordsRemoved)
wikiWords = cbind(wordsAdded, wordsRemoved)
library(caTools)
wikiWords = cbind(wikiWords, wiki$Vandal)
library(caTools)
set.seed(123)
spl = sample.split(wikiWords, SplitRatio = .7)
train = subset(wikiWords, spl == TRUE)
test = subset(wikiWords, spl == FALSE)
wikiWords = cbind(wordsAdded, wordsRemoved)
wikiWords
str(wikiWords)
table(test$wiki$Vandal)
table(test$`wiki$Vandal`)
table(test$`wiki$Vandal`)[1]/sum(table(test$`wiki$Vandal`))
str(wikiWords)
names(train$`wiki$Vandal`) ="Vandal"
names(test$`wiki$Vandal`)="Vandal"
modelCART = rpart(Vandal ~., data=train, method="class")
library(rpart)
modelCART = rpart(Vandal ~., data=train, method="class")
str(train)
colnames(train$`wiki$Vandal`) ="Vandal"
dim(train)
dim(test)
colnames(train)[329] ="Vandal"
colnames(test)[329] ="Vandal"
modelCART = rpart(Vandal ~., data=train, method="class")
pred = predict(modelCART, newdata = test)
pred[1:10, ]
pred.prob = pred[,2]
cm = table(test$Vandal, pred.prob >=0.5)
accuracy = (cm[1,1]+cm[2,2])/(cm[1,1]+cm[2,2]+cm[2,1]+cm[1,2])
accuracy #[1] 0.8560311
pred[1:10, ]
accuracy = sum(diag(cm))/sum(cm)
accuracy #[1] 0.8560311
prp(modelCART)
library(rpart.plot)
prp(modelCART)
set.seed
set.seed
set.seed(123)
spl = sample.split(wikiWords, SplitRatio = .7)
train = subset(wikiWords, spl == TRUE)
test = subset(wikiWords, spl == FALSE)
table(test$`wiki$Vandal`)[1]/sum(table(test$`wiki$Vandal`))
colnames(train)[329] ="Vandal"
colnames(test)[329] ="Vandal"
str(train)
modelCART = rpart(Vandal ~., data=train, method="class")
pred = predict(modelCART, newdata = test)
pred[1:10, ]
pred.prob = pred[,2]
cm = table(test$Vandal, pred.prob >=0.5)
accuracy = sum(diag(cm))/sum(cm)
accuracy #[1] 0.5463918
#Problem 1.7 - Bags of Words
prp(modelCART)
wikiWords = cbind(wordsAdded, wordsRemoved)
wikiWords = cbind(wikiWords, wiki$Vandal)
set.seed(123)
spl = sample.split(wikiWords, SplitRatio = .7)
train = subset(wikiWords, spl == TRUE)
test = subset(wikiWords, spl == FALSE)
table(test$`wiki$Vandal`)[1]/sum(table(test$`wiki$Vandal`))
colnames(train)[329] ="Vandal"
colnames(test)[329] ="Vandal"
modelCART = rpart(Vandal ~., data=train, method="class")
pred = predict(modelCART, newdata = test)
pred[1:10, ]
pred.prob = pred[,2]
cm = table(test$Vandal, pred.prob >=0.5)
accuracy = sum(diag(cm))/sum(cm)
accuracy #[1] 0.5463918
#Problem 1.7 - Bags of Words
prp(modelCART)
wikiWords2 = wikiWords
str(wikiWords2)
dim(wikiWords2)
dim(wiki)
wikiWords2$HTTP = ifelse(grepl("http",wiki$Added,fixed=TRUE), 1, 0)
table(wikiWords2$HTTP)
colnames(wikiWords2)[329] ="Vandal"
wikiTrain2 = subset(wikiWords2, spl==TRUE)
wikiTest2 = subset(wikiWords2, spl==FALSE)
modelCART = rpart(Vandal ~., data=wikiTrain2, method="class")
cm = table(test$Vandal, pred[,2] >=0.5)
accuracy = sum(diag(cm))/sum(cm)
accuracy #[1] 0.5463918
pred = predict(modelCART, newdata = wikiTest2)
cm = table(test$Vandal, pred[,2] >=0.5)
accuracy = sum(diag(cm))/sum(cm)
accuracy #[1] 0.5463918
set.seed(123)
spl = sample.split(wikiWords, SplitRatio = .7)
train = subset(wikiWords, spl == TRUE)
test = subset(wikiWords, spl == FALSE)
wikiTrain2 = subset(wikiWords2, spl==TRUE)
wikiTest2 = subset(wikiWords2, spl==FALSE)
modelCART = rpart(Vandal ~., data=wikiTrain2, method="class")
pred = predict(modelCART, newdata = wikiTest2)
head(pred)
cm = table(test$Vandal, pred[,2] >=0.5)
cm
cm = table(wikiTest2$Vandal, pred[,2] >=0.5)
cm
accuracy = sum(diag(cm))/sum(cm)
accuracy #[1] 0.5867698
table(wikiTest2$Vandal, pred[,2] >=0.5)
cm = table(wikiTest2$Vandal, pred[,2] >=0.5)
accuracy = sum(diag(cm))/sum(cm)
accuracy #[1] 0.5867698
accuracy =1
modelCART = rpart(Vandal ~., data=wikiTrain2, method="class")
pred = predict(modelCART, newdata = wikiTest2)
cm = table(wikiTest2$Vandal, pred[,2] >=0.5)
cm
accuracy = sum(diag(cm))/sum(cm)
accuracy #[1] 0.5867698
cm = table(wikiTest2$Vandal, pred, type="class")
pred = predict(modelCART, newdata = wikiTest2, type="class")
cm = table(wikiTest2$Vandal, pred)
accuracy = sum(diag(cm))/sum(cm)
accuracy #[1] 0.5867698
wikiCart2
wikiCart2 = rpart(Vandal ~., data=wikiTrain2, method="class")
wikiCart2 = rpart(Vandal ~., data=wikiTrain2, method="class")
pred = predict(wikiCart2, newdata = wikiTest2, type="class")
cm = table(wikiTest2$Vandal, pred)
accuracy = sum(diag(cm))/sum(cm)
accuracy #[1] 0.5867698
rm(list=ls())
setwd("/Users/saulgarcia/Desktop/Github/MIT_Analytics_Edge/Week5")
library(tm)
#Problem 1.1 - Bags of Words
wiki <- read.csv("wiki.csv", stringsAsFactors = FALSE)
wiki$Vandal <- as.factor(wiki$Vandal)
table(wiki$Vandal)[2]
#Problem 1.2 - Bags of Words
corpusAdded = Corpus(VectorSource(wiki$Added))
corpusAdded = tm_map(corpusAdded, removeWords, stopwords("english"))
corpusAdded = tm_map(corpusAdded, stemDocument)
dtmAdded = DocumentTermMatrix(corpusAdded)
dtmAdded
#Problem 1.3 - Bags of Words
#Filter out sparse terms by keeping only terms that appear in 0.3% or more of the revisions
sparseAdded = removeSparseTerms(dtmAdded, 0.997)
sparseAdded
#Problem 1.4 - Bags of Words
wordsAdded = as.data.frame(as.matrix(sparseAdded))
colnames(wordsAdded) = paste("A", colnames(wordsAdded))
#Repeat for removed words
corpusRemoved = Corpus(VectorSource(wiki$Removed))
corpusRemoved = tm_map(corpusRemoved, removeWords, stopwords("english"))
corpusRemoved = tm_map(corpusRemoved, stemDocument)
dtmRemoved = DocumentTermMatrix(corpusRemoved)
sparseRemoved = removeSparseTerms(dtmRemoved, 0.997)
wordsRemoved = as.data.frame(as.matrix(sparseRemoved))
colnames(wordsRemoved) = paste("R", colnames(wordsRemoved))
dim(wordsRemoved)
#Problem 1.5 - Bags of Words
wikiWords = cbind(wordsAdded, wordsRemoved)
wikiWords = cbind(wikiWords, wiki$Vandal)
colnames(wikiWords)[329] ="Vandal"
library(caTools)
set.seed(123)
spl = sample.split(wikiWords, SplitRatio = .7)
train = subset(wikiWords, spl == TRUE)
test = subset(wikiWords, spl == FALSE)
table(test$Vandal)[1]/sum(table(test$Vandal))
#Problem 1.6 - Bags of Words
library(rpart)
library(rpart.plot)
modelCART = rpart(Vandal ~., data=train, method="class")
pred = predict(modelCART, newdata = test)
pred[1:10, ]
pred.prob = pred[,2]
cm = table(test$Vandal, pred.prob >=0.5)
accuracy = sum(diag(cm))/sum(cm)
accuracy #[1] 0.5463918
#Problem 1.7 - Bags of Words
prp(modelCART)
prp(modelCART)
#Problem 2.1 - Problem-specific Knowledge
wikiWords2 = wikiWords
wikiWords2$HTTP = ifelse(grepl("http",wiki$Added,fixed=TRUE), 1, 0)
table(wikiWords2$HTTP)[2]
wikiTrain2 = subset(wikiWords2, spl==TRUE)
wikiTest2 = subset(wikiWords2, spl==FALSE)
wikiCart2 = rpart(Vandal ~., data=wikiTrain2, method="class")
pred = predict(wikiCart2, newdata = wikiTest2, type="class")
table(wikiTest2$Vandal, pred)
cm = table(wikiTest2$Vandal, pred)
accuracy = sum(diag(cm))/sum(cm)
accuracy #[1] 0.5867698
set.seed(111)
spl = sample.split(wikiWords, SplitRatio = .7)
train = subset(wikiWords, spl == TRUE)
test = subset(wikiWords, spl == FALSE)
table(test$Vandal)[1]/sum(table(test$Vandal))
#0.5343643
str(train)
#Problem 1.6 - Bags of Words
library(rpart)
library(rpart.plot)
modelCART = rpart(Vandal ~., data=train, method="class")
pred = predict(modelCART, newdata = test)
pred[1:10, ]
pred.prob = pred[,2]
cm = table(test$Vandal, pred.prob >=0.5)
accuracy = sum(diag(cm))/sum(cm)
accuracy #[1] 0.5463918
#Problem 1.7 - Bags of Words
prp(modelCART)
#Problem 2.1 - Problem-specific Knowledge
wikiWords2 = wikiWords
wikiWords2$HTTP = ifelse(grepl("http",wiki$Added,fixed=TRUE), 1, 0)
table(wikiWords2$HTTP)[2]
#Problem 2.2 - Problem-Specific Knowledge
wikiTrain2 = subset(wikiWords2, spl==TRUE)
wikiTest2 = subset(wikiWords2, spl==FALSE)
wikiCart2 = rpart(Vandal ~., data=wikiTrain2, method="class")
pred = predict(wikiCart2, newdata = wikiTest2, type="class")
cm = table(wikiTest2$Vandal, pred)
accuracy = sum(diag(cm))/sum(cm)
accuracy #[1] 0.5867698
cm
wikiWords2$NumWordsAdded = rowSums(as.matrix(dtmAdded))
head(wikiWords2$NumWordsAdded)
wikiWords2$NumWordsRemoved = rowSums(as.matrix(dtmRemoved))
mean(wikiWords2$NumWordsAdded)
wikiCart3 = rpart(Vandal ~., data=wikiTrain3, method="class")
wikiTrain3 = subset(wikiWords2, spl==TRUE)
wikiTest3 = subset(wikiWords2, spl==FALSE)
wikiCart3 = rpart(Vandal ~., data=wikiTrain3, method="class")
pred = predict(wikiCart2, newdata = wikiTest3, type="class")
cm = table(wikiTest3$Vandal, pred)
accuracy = sum(diag(cm))/sum(cm)
accuracy #[1] 0.5867698
predict(wikiCart2, newdata = wikiTest3, type="class")
pred = predict(wikiCart2, newdata = wikiTest3, type="class")
table(wikiTest3$Vandal, pred)
set.seed(111)
spl = sample.split(wikiWords, SplitRatio = .7)
wikiTrain3 = subset(wikiWords2, spl==TRUE)
wikiTest3 = subset(wikiWords2, spl==FALSE)
wikiCart3 = rpart(Vandal ~., data=wikiTrain3, method="class")
pred = predict(wikiCart2, newdata = wikiTest3, type="class")
cm = table(wikiTest3$Vandal, pred)
accuracy = sum(diag(cm))/sum(cm)
accuracy #[1]
set.seed(123)
spl = sample.split(wikiWords, SplitRatio = .7)
wikiTrain3 = subset(wikiWords2, spl==TRUE)
wikiTest3 = subset(wikiWords2, spl==FALSE)
wikiCart3 = rpart(Vandal ~., data=wikiTrain3, method="class")
pred = predict(wikiCart2, newdata = wikiTest3, type="class")
cm = table(wikiTest3$Vandal, pred)
accuracy = sum(diag(cm))/sum(cm)
accuracy #[1]
wikiWords3 = wikiWords2
wikiWords3$Minor = wiki$Minor
wikiWords3$Loggedin = wiki$Loggedin
wikiTrain4 = subset(wikiWords3, spl==TRUE)
wikiTest4 = subset(wikiWords3, spl==FALSE)
wikiCart4 = rpart(Vandal ~., data=wikiTrain4, method="class")
wikiCart3 = rpart(Vandal ~., data=wikiTrain3, method="class")
pred = predict(wikiCart3, newdata = wikiTest3, type="class")
cm = table(wikiTest3$Vandal, pred)
accuracy = sum(diag(cm))/sum(cm)
accuracy #[1] .6552021
pred = predict(wikiCart4)
pred = predict(wikiCart4, wikiTest4, type="class")
cm= pred(wikiTest4$Vandal, pred)
cm= table(wikiTest4$Vandal, pred)
cm
accuracy = sum(diag(cm))/sum(cm)
accuracy #[1] 0.6572165
prp(wikiCart4)
wikiWords2$NumWordsAdded = rowSums(as.matrix(dtmAdded))
wikiWords2$NumWordsRemoved = rowSums(as.matrix(dtmRemoved))
wikiWords3 = wikiWords2
wikiWords3$Minor = wiki$Minor
wikiWords3$Loggedin = wiki$Loggedin
wikiTrain4 = subset(wikiWords3, spl==TRUE)
wikiTest4 = subset(wikiWords3, spl==FALSE)
wikiCart4 = rpart(Vandal ~., data=wikiTrain4, method="class")
pred = predict(wikiCart4, wikiTest4, type="class")
cm= table(wikiTest4$Vandal, pred)
accuracy = sum(diag(cm))/sum(cm)
accuracy #[1] 0.7139175
#Problem 3.2 - Using Non-Textual Data
prp(wikiCart4)
rm(list=ls())
setwd("/Users/saulgarcia/Desktop/Github/MIT_Analytics_Edge/Week5")
library(tm)
library(caTools)
library(rpart)
library(rpart.plot)
trials <- read.csv("clinica_trial.csv", stringsAsFactors = FALSE)
getwd()
trials <- read.csv("clinical_trial.csv", stringsAsFactors = FALSE)
summary(trials)
str(trials)
length(trials$abstract[1])
nchar(trials$abstract[1])
trials$abstract[1]
trials$abstract[2]
nchar(trials$abstract[2])
max(nchar(trials$abstract))
noabstract = subset(trials, nchar(abstract)==0)
head(noabstract)
dim(subset(trials, nchar(abstract)==0))
dim(subset(trials, nchar(abstract)==0))[1]
which.min(nchar(trials$title))
trials$title[which.min(nchar(trials$title))]
corpusTitle = Corpus(VectorSource(trials$title))
corpusAbstract =Corpus(VectorSource(trials$abstract))
corpusTitle = tm(corpusTitle, tolower)
library(tm)
corpusTitle = tm(corpusTitle, tolower)
corpusTitle = tm_map(corpusTitle, tolower)
corpusAbstract = tm_map(corpusAbstract, tolower)
corpusTitle = tm_map(corpusTitle, PlainTextDocument)
corpusAbstract = tm_map(corpusAbstract, PlainTextDocument)
corpusTitle = tm_map(corpusTitle, removePunctuation)
corpusAbstract = tm_map(corpusAbstract, removePunctuation)
corpusTitle = tm_map(corpusTitle, removeWords, stopwords("english"))
corpusAbstract = tm_map(corpusAbstract, removeWords, stopwords("english"))
orpusTitle = tm_map(corpusTitle, stemDocument)
corpusTitle = tm_map(corpusTitle, stemDocument)
corpusAbstract = tm_map(corpusAbstract, stemDocument)
rm(orpusTitle)
dtmTitle = DocumentTermMatrix(corpusTitle)
dtmAbstract = DocumentTermMatrix(corpusAbstract)
dtmTitle = removeSparseTerms(dtmTitle, 0.95)
dtmAbstract = removeSparseTerms(dtmAbstract, .95)
dtmTitle = as.data.frame(as.matrix(dtmTitle))
dtmAbstract = as.data.frame(as.matrix(dtmAbstract))
dim(dtmTitle);dim(dtmAbstract)
colSums(dtmAbstract)
max(colSums(dtmAbstract))
which.max(colSums(dtmAbstract))
dtmAbstract[,which.max(colSums(dtmAbstract))]
colnames(dtmAbstract[,which.max(colSums(dtmAbstract))])
which.max(colSums(dtmAbstract))
which.max(colSums(dtmAbstract))
which.max(colSums(dtmAbstract))
colnames(dtmTitle) = paste0("T", colnames(dtmTitle))
head(dtmTitle)
colnames(dtmAbstract) = paste0("A", colnames(dtmAbstract))
dtm = cbind(dtmTitle, dtmAbstract)
dtm$trial = trials$trial
ncol(dtm)
spl = sample.split(dtm$trial, SplitRatio = .7)
set.seed(144)
spl = sample.split(dtm$trial, SplitRatio = .7)
train = subset(dtm, spl==TRUE)
test = subset(dtm, spl==FALSE)
Model1 = rpart(trial~., train, method="class")
pred = predict(Model1, newdata = test, type="class")
cm = table(test$trial, pred)
cm
accuracy = sum(diag(cm))/sum(cm)
accuracy
Baseline = table(test$trial)
Baseline
max(Baseline)/sum(Baseline)
prp(Model1)
prediction = predict(Model1)
head(prediction)
max(prediction[,2])
head(pred)
predtest = predict(Model1, newdata = test)
max(predtest[,2])
accuracy
cm
TN = cm[1,1]
TP = cm[2,2]
FN = cm[2,1]
FP = cm[1,2]
Sensitivity =  TP/(TP+FN)     #TP / Positives
Specificity =  TN/(TN + FP)   #TN / Negatives
Acc = (TP + TN)/sum(cm)
Sensitivity
Specificity
Acc
cm = table(train$trial, prediction[,2] >.5)
TN = cm[1,1]
TP = cm[2,2]
FN = cm[2,1]
FP = cm[1,2]
Sensitivity =  TP/(TP+FN)     #TP / Positives
Specificity =  TN/(TN + FP)   #TN / Negatives
Acc = (TP + TN)/sum(cm)
Sensitivity
Specificity
Acc
cm = table(test$trial, pred)
accuracy = sum(diag(cm))/sum(cm)
accuracy
library(ROCR)
pred = prediction(pred, test$trial)
pred = prediction(pred, test$trial)
prediction(pred, test$trial)
pred = predict(Model1, newdata = test)
predi = prediction(pred, test$trial)
as.numeric(performance(predi,"auc")@y.values)
pred = predict(Model1, newdata = test)
pred = prediction(pred, test$trial)
pred = prediction(pred[,2], test$trial)
as.numeric(performance(pred,"auc")@y.values)
?Random
random()
random()
