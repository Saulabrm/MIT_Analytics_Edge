where
t.in_use = True
and t.id::varchar(100) not in (
select distinct ip_id::varchar(100)
from twitter.keywords
)
;"
# Retreives the table from the database
names <- dbGetQuery(con, query_kw)
if(nrow(names)!=0){
unwanted_array = list(    'Š'='S', 'š'='s', 'Ž'='Z', 'ž'='z', 'À'='A', 'Á'='A', 'Â'='A', 'Ã'='A', 'Ä'='A', 'Å'='A', 'Æ'='A', 'Ç'='C', 'È'='E', 'É'='E',
'Ê'='E', 'Ë'='E', 'Ì'='I', 'Í'='I', 'Î'='I', 'Ï'='I', 'Ñ'='N', 'Ò'='O', 'Ó'='O', 'Ô'='O', 'Õ'='O', 'Ö'='O', 'Ø'='O', 'Ù'='U',
'Ú'='U', 'Û'='U', 'Ü'='U', 'Ý'='Y', 'Þ'='B', 'ß'='Ss', 'à'='a', 'á'='a', 'â'='a', 'ã'='a', 'ä'='a', 'å'='a', 'æ'='a', 'ç'='c',
'è'='e', 'é'='e', 'ê'='e', 'ë'='e', 'ì'='i', 'í'='i', 'î'='i', 'ï'='i', 'ð'='o', 'ñ'='n', 'ò'='o', 'ó'='o', 'ô'='o', 'õ'='o',
'ö'='o', 'ø'='o', 'ù'='u', 'ú'='u', 'û'='u', 'ý'='y', 'ý'='y', 'þ'='b', 'ÿ'='y' )
#Remove Stopwords
stopwordsFr<-stopwords(kind="french")
stopwordsEn<-stopwords(kind="en")
stopWords<- c(stopwordsEn,stopwordsFr,"lyon")
names.lower<- as.data.frame(lapply(names, tolower))
names.lower$name<- as.character(names.lower$name)
names.lower$id <- as.character(names.lower$id)
#Clean
for(i in 1:nrow(names.lower)){
#Get rid of punctuations
names.lower[i,2]<-gsub('[[:punct:]]', " ", names.lower[i,2])
#Get rid of symbols
names.lower[i,2]<-gsub('[[:cntrl:]]', " ", names.lower[i,2])
#Get rid of the accents
names.lower[i,2]<-chartr(paste(names(unwanted_array), collapse=''),
paste(unwanted_array, collapse=''),
names.lower[i,2])
}
######## Remove StopWords #####################
'%nin%' <- Negate('%in%')
X<-lapply(names.lower[,2], function(x) {
t <- unlist(strsplit(x, " "))
t[t %nin% stopWords]
})
#Remove extra spaces
names.lower[,2]<- sapply(X, paste, collapse=",")
keywords = names.lower[,c(1,2)]
#Remove extra character
for(i in 1:nrow(keywords)){
keywords[i,2]<-gsub(",,,", ",", keywords[i,2])
}
names(keywords) = c("ip_id","keywords")
###Insert to DB
insert <- function(i, con, keywords) {
txt <- paste("INSERT into twitter.keywords values (",keywords$ip_id[i],", '",keywords$keywords[i],"');")
dbGetQuery(con, txt)
}
for (i in 1:length(keywords$ip_id)){
insert(i, con, keywords)
}
}
rm(query_kw)
############    KEYWORDS   #################
############################################
#####################################################
############# MODEL TWEETS_TO_IP   ##################
# Queries to retrieve combinations of Tweets - Kewyords
#Keywords
query_kw <- "
select
k.ip_id,
k.keyword
from
twitter.keywords k;"
#Tweets
query_t <- "
select
t.idd::varchar(100),
t.text
from
twitter.tweets t;"
#Processed
query_p <-"
select distinct tweet_id::varchar(100)
from twitter.processed_tweets"
#Get data and turn it to data table
df1 <- data.table(dbGetQuery(con, query_kw))
df2 <- data.table(dbGetQuery(con, query_t))
df3 <- data.table(dbGetQuery(con, query_p))
setkey(df1,c(ip_id,keyword))
setkey(df2, idd)
setkey(df3, tweet_id)
#Subset the not processed tweets
df <- df2[-df2[df3, which=TRUE],][1:2000]
#Prepare to update the processed tweets
processed_tweets = as.data.frame(unique(df$idd))
names(processed_tweets) = "idd"
processed_tweets$idd = as.character(processed_tweets$idd)
trigger = nrow(processed_tweets)
#Clean Text
df$text_clean = clean_text(df$text)
#Combinations for Tweets to IP
df <- merge(as.data.frame(df),as.data.frame(df1), all.x = TRUE, all.y = TRUE)
#Count how many keywords appear in text
df$count = count_substring(df$text_clean, df$keyword)
#Only tweets to ip if there are relations
if(sum(df$count)>0){
#tweet_to_ip = df %>% filter(count > 0) %>% select(idd, ip_id)
tweet_to_ip = subset(df,count>0)[,c(1,4)]
names(tweet_to_ip) = c("twitter_id","ip_id")
tweet_to_ip<- tweet_to_ip[,c(2,1)]
insert <- function(i, con, tweet_to_ip) {
txt <- paste("INSERT into twitter.tweet_to_ip values (",tweet_to_ip$ip_id[i],", ",tweet_to_ip$twitter_id[i],"::bigint);")
dbGetQuery(con, txt)
}
for (i in 1:length(tweet_to_ip$ip_id)){
insert(i, con, tweet_to_ip)
}
}
#Record Processed Tweets
if(trigger[[1]]>0){ #Run if there are tweets
insert <- function(i, con, processed_tweets) {
txt <- paste("INSERT into twitter.processed_tweets (tweet_id, processed_date) VALUES (",processed_tweets$idd[i],",  now() );")
dbGetQuery(con, txt)
}
for (i in 1:length(processed_tweets$idd)){
insert(i, con, processed_tweets)
}
}
#########Close PostgreSQL connection###############
dbDisconnect(con)
rm(list=ls())
date()
#KeywordsTweets_to_IP
songs <- read.csv("songs.csv")
#setwd("/Users/saulgarcia/Desktop/Github/MOOCs/MIT Analyticals Edge/Week3")
songs <- read.csv("songs.csv")
setwd("/Users/saulgarcia/Desktop/Github/MOOCs/MIT Analyticals Edge/Week3")
songs <- read.csv("songs.csv")
setwd("/Users/saulgarcia/Desktop/Github/MOOCs/MIT_Analytics_Edge/Week3")
setwd("/Users/saulgarcia/Desktop/Github/MOOCs/MIT_AnalyticsEdge/Week3")
setwd("/Users/saulgarcia/Desktop/Github/MIT_Analytics_Edge/Week3")
#### 1 - POPULARITY OF MUSIC RECORDS ####
songs <- read.csv("songs.csv")
nrow(songs)
str(songs)
nrow(songs[songs$year==2010,])
nrow(songs[songs$artistname=="Michael Jackson",])
nrow(songs[songs$artistname=="Michael Jackson" & songs$Top10=="1",])
songs[songs$artistname=="Michael Jackson" & songs$Top10=="1",]
songs[songs$artistname=="Michael Jackson" & songs$Top10=="1",][2,]
songs[songs$artistname=="Michael Jackson" & songs$Top10=="1",][,2]
songs[songs$artistname=="Michael Jackson" & songs$Top10=="1",]
table(songs$timesignature)
which.max(songs$tempo)
songs[which.max(songs$tempo),2]
SongsTrain <- subset(songs, year=< 2009)
SongsTrain <- subset(songs, year<= 2009)
SongsTest <-  subset(songs, year==2010)
nrow(SongsTrain)
nonvars <- c("year","songtitle", "artistname", "songID", "artistID")
SongsTrain <- SongsTrain[ ,!(names(SongsTrain) %in% nonvars)]
SongsTest <- SongsTest[ ,!(names(SongsTest) %in% nonvars)]
names(SongsTest)
SongsLog1 = glm(Top10 ~ ., data=SongsTrain, family=binomial)
summary(SongsLog1)
summary(SongsLog1)%AIC
summary(SongsLog1)$AIC
summary(SongsLog1)$aic
cor(SongsTrain$loudness,SongsTrain$energy)
SongsLog2 = glm(Top10 ~ . - loudness, data=SongsTrain, family=binomial)
summary(SongsLog2)
summary(SongsLog2)
summary(SongsLog1)
SongsLog3 = glm(Top10 ~ . - energy, data=SongsTrain, family=binomial)
summary(SongsLog3)
GLMPrediction = predict(SongsLog3, type="response", newdata=SongsTest)
GLMprediction = predict(SongsLog3, type="response", newdata=SongsTest)
LMprediction
GLMprediction
table(SongsTest$Top10, GLMPrediction > .45)
table(SongsTest$Top10, GLMPrediction => .45)
table(SongsTest$Top10, GLMPrediction >= .45)
cm = table(SongsTest$Top10, GLMPrediction > .45)
diag(cm)
sum(diag(cm))
accuracy = sum(diag(cm))/ sum(cm)
accuracy
table(SongsTest$Top10)
=59/314
59/314
table(SongsTest$Top10)[1]/sum(table(SongsTest$Top10))
cm
cm[2,2]
cm[1,2]
TN = cm[1,1]
TP = cm[2,2]
FN = cm[2,1]
FP = cm[1,2]
Sensitivity =  TP/(TP+FN)     #TP / Positives
Specificity =  TN/(TN + FP)   #TN / Negatives
Sensitivity
Specificity
parole <- read.csv("parole.csv")
nrow(parole)
table(parole$violator)
table(parole$violator)[2]
str(parole)
summary(parole)
parole$state <- as.factor(parole$state)
parole$crime <- as.factor(parole$crime)
summary(parole)
set.seed(144)
library(caTools)
split =  sample.split(parole$violator, SplitRatio = 0.7)
train <- subset(parole,split==TRUE)
test <- subset(parole,split==FALSE)
head(train)
split =  sample.split(parole$violator, SplitRatio = 0.7)
train <- subset(parole,split==TRUE)
head(train)
set.seed(144)
split =  sample.split(parole$violator, SplitRatio = 0.7)
train <- subset(parole,split==TRUE)
head(train)
set.seed(144)
library(caTools)
split =  sample.split(parole$violator, SplitRatio = 0.7)
train <- subset(parole,split==TRUE)
head(train)
split =  sample.split(parole$violator, SplitRatio = 0.7)
train <- subset(parole,split==TRUE)
head(train)
set.seed(144)
library(caTools)
split =  sample.split(parole$violator, SplitRatio = 0.7)
train <- subset(parole,split==TRUE)
test <- subset(parole,split==FALSE)
model = glm(violator~. , train, family = "binomial")
summary(model)
set.seed(144)
split =  sample.split(parole$violator, SplitRatio = 0.7)
train <- subset(parole,split==TRUE)
test <- subset(parole,split==FALSE)
model = glm(violator~. , train, family = "binomial")
summary(model)
exp(1.61)
-4.2411574 + exp(0.3869904) + 50*exp(-0.0001756) + 3*exp(0.0802954)+ 2*exp(0.6837143)
exp(-4.2411574 + 0.3869904 + 50*-0.0001756 + 3*0.0802954+ 2*0.6837143)
exp(-4.2411574 + 0.3869904 + 50*-0.0001756 + 3*-0.1238867 +12*0.0802954+ 2*0.6837143)
head(test)
test = rbind(test,c(1,1,50,1,3,12,0,2,0))
predict(model,test[203,],type=response)
predict(model,test[203,],type="response")
exp(-4.2411574 + 0.3869904 +0.8867192 + 50*-0.0001756 + 3*-0.1238867 +12*0.0802954+ 2*0.6837143)
prediction <- predict(model,test[-203,],type="response")
max(prediction)
table(test$violator, prediction > .5)
table(test$violator[-203], prediction > .5)
cm = table(test$violator[-203], prediction > .5)
TN = cm[1,1]
TP = cm[2,2]
FN = cm[2,1]
FP = cm[1,2]
Sensitivity =  TP/(TP+FN)     #TP / Positives
Specificity =  TN/(TN + FP)   #TN / Negatives
Accuracy = sum(diag(cm))/sum(cm)
Sensitivity =  TP/(TP+FN)     #TP / Positives
Specificity =  TN/(TN + FP)   #TN / Negatives
Accuracy = sum(diag(cm))/sum(cm)
Sensitivity
Specificity
Accuracy
table(test$violator)
table(test$violator[-203])
179/202
table(test$violator[-203], prediction > .7)
table(test$violator[-203], prediction > .4)
table(test$violator[-203], prediction > .2)
Accuracy
179/202
summary(model)
library(ROCR)
ROCRpred = prediction(prediction, test$violator[-203])
ROCRperf = performance(ROCRpred, "tpr","fpr")
plot(ROCRperf)
plot(ROCRperf, colorize = TRUE)
plot(ROCRperf, colorize = TRUE, print.cutoffs.at=seq(0,1,0.1), text.adj=c(-.2,1.7))
plot(ROCRperf)
plot(ROCRperf, colorize = TRUE)
ROCRpred
ROCRperf
auc = as.numeric(performance(ROCRpred, "auc")@y.values)
auc
loans <- read.csv("loans.csv")
str(loans)
table(loans$not.fully.paid)
table(loans$not.fully.paid)[2]/sum(table(loans$not.fully.paid))
is.na(loans)
colSums(is.na(loans))
colnames(loans)[colSums(is.na(loans)) > 0]
x=subset(loans, na.rm)
x=subset(loans, is.na(loans) == T)
x
x=subset(loans, is.na(loans) == F)
missing = subset(loans, is.na(log.annual.inc) | is.na(days.with.cr.line) | is.na(revol.util) | is.na(inq.last.6mths) | is.na(delinq.2yrs) | is.na(pub.rec))
library(mice)
set.seed(144)
vars.for.imputation = setdiff(names(loans), "not.fully.paid")
vars.for.imputation
imputed = complete(mice(loans[vars.for.imputation]))
loans[vars.for.imputation] = imputed
imputed
summary(imputed)
summary(loans)
setdiff(names(loans), "not.fully.paid")
names(loans)
?mice
set.seed(144)
split = sample.split(loans$not.fully.paid, SplitRatio = 0.7)
loansimputed <- read.csv("loans.imputed.csv")
loansimputed <- read.csv("loans_imputed.csv")
set.seed(144)
split = sample.split(loansimputed$not.fully.paid, SplitRatio = 0.7)
train = subset(loansimputed, split == TRUE)
test = subset(loansimputed, split == FALSE)
loansGLM = glm(not.fully.paid~., family = "binary", data = train)
loansGLM = glm(not.fully.paid~., family = "binomial", data = train)
summary(loansGLM)
summary(loansGLM)$coef
summary(loansGLM)$coef[2,2]
summary(loansGLM)$coef[1,2]
summary(loansGLM)$coef[1,1]
summary(loansGLM)$coef[13,1]
exp(700*summary(loansGLM)$coef[13,1]) - exp(710*summary(loansGLM)$coef[13,1])
exp(700*summary(loansGLM)$coef[13,1]) / exp(710*summary(loansGLM)$coef[13,1])
quality <- read.csv("quality.csv")
str(quality)
table(quality$PoorCare) #See how many people received Poorcare
#Baseline model
98/131 #We ware trying to beat this!
#Split data into Training and Testing
set.seed(88)
split = sample.split(quality$PoorCare, SplitRatio = .75)  #Splits intellegently
split #True means trainingset, False means Testingset
qualityTrain = subset(quality, split==TRUE)
qualityTest = subset(quality, split==FALSE)
nrow(qualityTrain)
nrow(qualityTest)
QualityLog = glm(PoorCare ~ OfficeVisits + Narcotics, data = qualityTrain, family = binomial)
summary(QualityLog)
#It is important to see AIC, it is equivalent to R squared in Linear Regression.. (only be compared between models with the same dataset.)
predictTrain = predict(QualityLog, type="response")
summary(predictTrain)
tapply(predictTrain, qualityTrain$PoorCare, mean)
QualityLog2 = glm(PoorCare ~ StartedOnCombination + ProviderCount, data = qualityTrain, family= binomial)
summary(QualityLog2)
QualityLog2$coefficients[2]
str(loans)
table(loans$not.fully.paid)
table(loans$not.fully.paid)[2]/sum(table(loans$not.fully.paid))
#Problem 1.2 - Preparing the Dataset
colSums(is.na(loans))
colnames(loans)[colSums(is.na(loans)) > 0]
#Problem 1.3 - Preparing the Dataset
missing = subset(loans, is.na(log.annual.inc) | is.na(days.with.cr.line) | is.na(revol.util) | is.na(inq.last.6mths) | is.na(delinq.2yrs) | is.na(pub.rec))
#We want to be able to predict risk for all borrowers, instead of just the ones with all data reported.
#Problem 1.4 - Preparing the Dataset
library(mice)
set.seed(144)
vars.for.imputation = setdiff(names(loans), "not.fully.paid")
imputed = complete(mice(loans[vars.for.imputation]))
loans[vars.for.imputation] = imputed
#Problem 2.1 - Prediction Models
loansimputed <- read.csv("loans_imputed.csv")
set.seed(144)
split = sample.split(loansimputed$not.fully.paid, SplitRatio = 0.7)
train = subset(loansimputed, split == TRUE)
test = subset(loansimputed, split == FALSE)
loansGLM = glm(not.fully.paid~., family = "binomial", data = train)
summary(loansGLM)
predicted.risk = glm(not.fully.paid~., family = "binomial", data = train)
700*summary(loansGLM)$coef[13,1]) - 710*summary(loansGLM)$coef[13,1]
700*summary(loansGLM)$coef[13,1])
loansGLM = glm(not.fully.paid~., family = "binomial", data = train)
700*summary(loansGLM)$coef[13,1]) - 710*summary(loansGLM)$coef[13,1]
summary(loansGLM)
summary(loansGLM)$coef[13,1])
700*summary(loansGLM)$coef[13,1] - 710*summary(loansGLM)$coef[13,1]
exp(700*summary(loansGLM)$coef[13,1] - 710*summary(loansGLM)$coef[13,1])
predicted.risk = predict(loansGLM, test )
cm = table(test$not.fully.paid, predicted.risk > .5)
table(test$not.fully.paid, predicted.risk > .5)
Accuracy= sum(diag(cm))/sum(cm)
Accuracy
table(test$not.fully.paid)
table(test$not.fully.paid)[1]/sum(table(test$not.fully.paid))
ROCRpred = prediction(predicted.risk, test$not.fully.paid)
ROCRperf = performance(ROCRpred, "tpr","fpr")
auc = as.numeric(performance(ROCRpred, "auc")@y.values)
auc
plot(ROCRperf)
plot(ROCRperf, colorize = TRUE)
model2 = glm(not.fully.paid ~ int.rate, family="binomial", train)
summary(model2)
prediction = predict(model2, test)
max(prediction)
min(prediction)
model2 = glm(not.fully.paid ~ int.rate, family="binomial", train)
summary(model2)
#int.rate is correlated with other risk-related variables, and therefore does not incrementally improve the model when those other variables are included.
#Problem 3.2 - A "Smart Baseline"
prediction = predict(model2, test)
prediction
max(predicted.risk)
table(test$not.fully.paid, prediction > .5)
prediction = predict(model2, test, type="response")
ROCRpred = prediction(predicted.risk, test$not.fully.paid, type="response")
prediction = predict(model2, test, type="response")
max(prediction)
table(test$not.fully.paid, prediction > .5)
prediction = predict(model2, test, type="response")
max(prediction)
table(test$not.fully.paid, prediction > .5)
min(prediction)
max(prediction)
table(test$not.fully.paid, prediction > .5)
ROCRpred = prediction(prediction, test$not.fully.paid)
ROCRperf = performance(ROCRpred, "tpr","fpr")
auc = as.numeric(performance(ROCRpred, "auc")@y.values)
auc
10 * exp(.06*3)
10 * exp(.06*3-1)
test$profit = exp(test$int.rate*3) - 1
test$profit[test$not.fully.paid == 1] = -1
head(test$profit)
max(test$profit*10)
sub <- subset(test, int.rate > .15)
mean(test$profit*1)
mean(sub$profit*1)
table(sub$not.fully.paid)
table(sub$not.fully.paid)[2]/sum(table(sub$not.fully.paid))
names(sub)
predicted.risk = predict(loansGLM, sub, type="response")
sub$predicted.risk = predicted.risk
cutoff = sort(sub$predicted.risk, decreasing=FALSE)[100]
cutoff
summary(sub)
sub <- subset(test, int.rate > .15)
cutoff = sort(sub$predicted.risk, decreasing=FALSE)[100]
#Problem 6.2 - An Investment Strategy Based on Risk
predicted.risk = predict(loansGLM, sub, type="response")
sub$predicted.risk = predicted.risk
cutoff = sort(sub$predicted.risk, decreasing=FALSE)[100]
selLoans = subset(sub, predicted.risk <= cutoff)
sum(selLoans$profit)
min(selLoans$profit)
table(selLoans$not.fully.paid)[2]/sum(table(selLoans$not.fully.paid))
table(selLoans$not.fully.paid
)
table(selLoans$not.fully.paid)[2]
parole <- read.csv("parole.csv")
#Problem 1.1 - Loading the Dataset
nrow(parole)
#Problem 1.2 - Loading the Dataset
table(parole$violator)[2]
#Problem 2.1 - Preparing the Dataset
summary(parole)
#state and crime
#Problem 2.2 - Preparing the Dataset
parole$state <- as.factor(parole$state)
parole$crime <- as.factor(parole$crime)
summary(parole)
#Output like table()
#Problem 3.1 - Splitting into a Training and Testing Set
set.seed(144)
library(caTools)
split =  sample.split(parole$violator, SplitRatio = 0.7)
train <- subset(parole,split==TRUE)
test <- subset(parole,split==FALSE)
#70 and 30%
#Problem 3.2 - Splitting into a Training and Testing Set
#Exact if I run the 5 lines again, Different if I run them again without seed, Different if I use a different seed.
#Problem 4.1 - Building a Logistic Regression Model
model = glm(violator~. , train, family = "binomial")
summary(model)
#Problem 4.2 - Building a Logistic Regression Model
exp(1.61)
test = rbind(test,c(1,1,50,1,3,12,0,2,0))
predict(model,test[203,],type="response")
exp(-4.2411574 + 0.3869904 +0.8867192 + 50*-0.0001756 + 3*-0.1238867 +12*0.0802954+ 2*0.6837143)
-4.2411574 + 0.3869904 +0.8867192 + 50*-0.0001756 + 3*-0.1238867 +12*0.0802954+ 2*0.6837143
summary(model)
exp(-4.2411574 + 0.3869904 +0.8867192 + 50*-0.0001756 + 3*-0.1238867 +12*0.0802954+ 2*0.6837143)
test = rbind(test,c(1,1,50,1,3,12,0,2,0))
predict(model,test[203,],type="response")
-4.2411574 + 0.3869904
-4.2411574 + 0.3869904 +0.8867192 + 50*-0.0001756 + 3*-0.1238867 +12*0.0802954+ 2*0.6837143
-4.2411574 + 0.3869904 +0.8867192 + 50*-0.0001756 + 3*-0.1238867 +12*0.0802954+ 1*0.6837143
e
exp(-4.2411574 + 0.3869904 +0.8867192 + 50*-0.0001756 + 3*-0.1238867 +12*0.0802954+ 1*0.6837143)
Probability = 1/(1+Odds)
1/(1+Odds)
Odds=exp(-4.2411574 + 0.3869904 +0.8867192 + 50*-0.0001756 + 3*-0.1238867 +12*0.0802954+ 1*0.6837143)
1/(1+Odds)
1/(1+Odds)
1+Odds
1/(1+exp(1.700629))
1/(1+Odds)
(1+exp(1.700629))
1+Odd
exp(1.700629)
exp(-4.2411574 + 0.3869904 +0.8867192 + 50*-0.0001756 + 3*-0.1238867 +12*0.0802954+ 1*0.6837143)
Probability = 1/(1+exp(1.700629))
150*617 + 16*238
rm(list=ls())
Empirical Study of Machine Learning Based Approach  for Opinion Mining in Tweets
